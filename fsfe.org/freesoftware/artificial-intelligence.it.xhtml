<?xml version="1.0" encoding="utf-8"?>
<html>
<version>1</version>

<head>
  <title>Controllare la tecnologia nell'era dell'Intelligenza Artificiale: una prospettiva dal lato del Software Libero</title>
</head>
<body class="article">
  <h1>Controllare la tecnologia nell'era dell'Intelligenza Artificiale: una prospettiva dal lato del Software Libero</h1>
  <div id="introduction">
    <p>Miglioramenti tecnici, l'accumulo di un gran numero di dati dettagliati e l'avanzamento nelle prestazioni hardware dei computer hanno portato ad una rivoluzione dell'Intelligenza Artificiale (IA). Ad esempio, una grande progresso nella visione artificiale ha permesso di prendere decisioni automatiche sulla base di immagini e video, e la costruzione di un'ampia base dati e il miglioramento dell'analisi testuale combinate con la raccolta di dati personali hanno fatto nascere un'innumerevole quantità di applicazioni IA. Queste nuove applicazioni IA hanno portati molti benefici ai cittadini dell'Unione Europea (UE). Ma, data la sua insita complessità e i suoi requisiti a riguardo delle risorse e della conoscenza tecnica, l'IA può pregiudicare la nostra capacità di controllare la tecnologia e mette a rischio le libertà fondamentali. È quindi utile fissarsi come obiettivo quello di introdurre nuove leggi sull'IA.</p>

    <p>Nel contesto di una nuova legislazione, questo articolo spiega come il rilascio delle applicazioni IA con licenze di Software Libero possa spianare la strada per una maggiore accessibilità, trasparenza ed imparzialità.
    </p>
  </div>

  <h2 id="freesoftware">Cos'è il Software Libero?</h2>

  <p>Il <a href="https://webpreview.fsfe.org/cgi-bin/freesoftware.html">Software Libero</a> (conosciuto anche come Open Source)
aiuta le persone a controllare la tecnologia garantendo ad ogni utente quattro libertà:
  </p>
  <ol>
    <li>La libertà di usare il software per ogni scopo, senza alcuna limitazione geografica.
    </li>
    <li>
      La libertà di studiare il software, senza accordi di non divulgazione.
    </li>
    <li>La libertà di condividere il software e copiarlo a costo zero.</li>
    <li>La libertà di perfezionare il software e di condividerne i miglioramenti.</li>
  </ol>

  <p>Queste libertà sono garantite rilasciando il software con una licenza di Software Libero, i cui termini sono compatibili con le suddette libertà. Ci sono molteplici licenze di Software Libero per diversi scopi. Un software può essere rilasciato con più di una licenza. Per poter essere modificata liberamente, una IA richiede che venga fornito sia il codice di apprendimento che i dati, ed entrambi devono essere rilasciati con una licenza di Software Libero affinché l'IA venga considerata libera.</p>

  <h2 id="accessibility">Accessibilità</h2>

  <p>L'accessibilità per l'IA significa renderla riutilizzabile, così che ciascuno possa personalizzarla, migliorarla e utilizzarla per i propri scopi. Per rendere l'IA riutilizzabile, occorre che venga rilasciata con una licenza di Software Libero. Sono moltissimi i vantaggi di questo approccio. Avendo delle basi legali aperte, una IA libera promuove l'innovazione, perché non si ha a che fare con restrizioni artificiose che impediscono alle persone di riutilizzare del lavoro già fatto. Rendere l'IA libera evita quindi dal dover reinventare ogni volta la ruota, permettendo allo stesso tempo ai ricercatori e agli sviluppatori di focalizzarsi nel creare software IA nuovo e migliore invece di ricostruire e ricreare ripetutamente i blocchi e il lavoro già fatto. In aggiunta al miglioramento dell'efficienza condividendo la competenza, le IA libere abbassano anche i costi di sviluppo risparmiando tempo ed eliminando i costi di licenza. Tutto questo migliora l'accessibilità dell'IA, che porta a soluzioni migliori e più democratiche dal momento che tutti possono parteciparvi.</p>

  <p>Rendere l'IA riutilizzabile fa diventare anche più semplice costruire modelli IA specializzati partendo da quelli più generici. Se un modello IA generico venisse rilasciato come Software Libero, piuttosto che creare un nuovo modello da zero si potrebbe utilizzare il modello generico come punto di partenza per uno specifico compito base (downstream task) per la previsione. Ad esempio, si potrebbe utilizzare un modello generico di visione artificiale<a href="#fn-1" id="ref-1" class="fn">1</a><span class="fn">,</span><a href="#fn-2" id="ref-2" class="fn">2</a> come punto di partenza per gestire l'infrastruttura pubblica che richiede elaborazioni di immagine specifici. Semplicemente come l'accessibilità in generale, questo approccio ha un vantaggio principale: i modelli generici con molti parametri ed addestrati su un grande insieme di dati potrebbero rendere il compito base più semplice da apprendere. Questo rende l'IA più accessibile abbassando la soglia di ingresso, rendendone più semplice il suo riutilizzo.</p>

  <p>Rendere liberi sia il codice sorgente per addestrare l'IA che i dati corrispondenti potrebbe non essere sufficiente per rendere l'IA accessibile. L'IA richiede un enorme mole di dati per identificare gli schemi e le correlazioni che portano a delle corrette previsioni. Al contrario, non avere abbastanza dati reduce la sua capacità di capire il mondo circostante. Inoltre, insiemi di dati numerosi e la loro relativa complessità tendono ad ingigantire i modelli IA, rendendo il loro l'addestramento dispendioso in termini di tempo e di risorse. La complessità nel gestire i dati richiesti per addestrare i modelli IA, insieme alla conoscenza richiesta per svilupparli e la gestione delle capacità computazionali, richiede molto capitale umano. Può essere quindi difficile avvalersi delle libertà offerta da una IA libera, anche quando il codice sorgente e i dati per addestrarla venissero rilasciati come Software Libero. In questi casi, rilasciare i modelli già addestrati come Software Libero migliorerebbe di molto l'accessibilità.</p>
  <p>Infine, occorrerebbe notare che, come ogni altra tecnologia, rendere l'IA riutilizzabile da chiunque può essere potenzialmente dannoso. Ad esempio, riutilizzare un rilevatore di volti rilasciato come Software Libero come parte di un software di riconoscimento facciale può causare problemi per i diritti umani. Questo è comunque vero a prescindere dalla tecnologia utilizzata. Se un utilizzo particolare di un software è ritenuto dannoso, dovrebbe essere proibito senza un'esplicita messa al bando della tecnologia IA.</p>

  <h2 id="transparency">Trasparenza</h2>

  <p>La trasparenza dell'IA può essere suddivisa in apertura e interpretabilità. In questo contesto, l'apertura è definita come il diritto di essere informati sul software IA, e l'interpretabilità è definita come essere in grado di capire come i dati in ingresso vengono processati in modo che si possano identificare i fattori presi in considerazione per fare le previsioni e la loro relativa importanza. In Europa, il diritto di essere informati sulle decisioni di un algoritmo è garantito dal considerando 71 del Regolamento Generale sulla Protezione dei Dati (GDPR) 2016/679: «<em>In ogni caso, tale trattamento dovrebbe essere subordinato a garanzie adeguate, che dovrebbero comprendere la specifica informazione all'interessato e il diritto di ottenere l'intervento umano, di esprimere la propria opinione, di ottenere una spiegazione della decisione conseguita dopo tale valutazione e di contestare la decisione</em>». La trasparenza può essere quindi definita come la capacità di capire da cosa sono guidate le previsioni.
  </p>

  <p>L'IA ha bisogno di essere trasparente perché è utilizzata per questioni critiche. Ad esempio, è utilizzata per determinare la solvibilità<a href="#fn-3" id="ref-3" class="fn">3</a>, nelle auto a guida autonoma<a href="#fn-4" id="ref-4" class="fn">4</a>, nelle previsioni delle attività criminali<a href="#fn-5" id="ref-5" class="fn">5</a> o nell'assistenza sanitaria<a href="#fn-6" id="ref-6" class="fn">6</a>. In questi contesti, è quindi fondamentale avere le informazioni su come queste previsioni sono fatte, e dovrebbero essere rese disponibili le informazioni sui dati utilizzati e come questi vengono processati dall'IA. In questo caso, la fiducia e l'adozione dell'IA sarebbero di conseguenza maggiori. Inoltre, le moderne tecnologie IA, come l'apprendimento profondo, non sono pensate per essere trasparenti, perché sono composte da milioni o miliardi di singoli parametri<a href="#fn-7" id="ref-7" class="fn">7</a>, rendendole molto complesse e difficili da capire. In questo ambito è quindi dovuto l'utilizzo del Software Libero, attraverso il quale è possibile cercare di analizzare queste complessità.
  </p>
  <p>Esistono già tecnologie rilasciate come Software Libero per rendere l'IA più trasparente. Ad esempio, Local Interpretable Model-Agnostic Explanations
    (LIME)<a href="#fn-8" id="ref-8" class="fn">8</a> è un pacchetto software che semplifica un modello di previsione complesso simulandolo con una versione più semplice e maggiormente interpretabile, e quindi permette agli utenti dell'IA di capire quali parametri giocano un ruolo nella previsione. La figura 1 mostra questo processo confrontando le previsioni fatte da due modelli diversi. Captum<a href="#fn-9" id="ref-9" class="fn">9</a> è una libreria rilasciata come Software Libero che fornisce un meccanismo di attribuzione per capire l'importanza relativa di ciascuna delle variabili di ingresso e di ogni parametro di un modello ad apprendimento profondo. È quindi fattibile rendere l'IA più trasparente.
  </p>
  <figure>
    <img src="https://pics.fsfe.org/uploads/big/79c895a423457c9b60b4106a2401d631.png"/>
    <figcaption>Figura 1: esempio di spiegazioni delle previsioni in LIME<a href="#fn-8" id="ref-8" class="fn">8</a></figcaption>
  </figure>
  <p>Anche se una IA proprietaria può essere trasparente, il Software Libero aiuta questo processo rendendo la verifica e il controllo più semplici. Nonostante alcuni dati potrebbero essere troppo sensibili per essere rilasciati con una licenza di Software Libero, le proprietà statistiche dei dati possono essere pubblicate. Con il Software Libero, ognuno è in grado di far funzionare l'IA per capire come è fatta, e vedere i dati che vengono da essa elaborati. Occorre però notare che il modello IA, composto da milioni o miliardi di parametri, non è fatto per essere trasparente, ma, simulando il modello IA con uno molto più semplice, può essere più facile controllarlo.</p>
  <p>Un altro beneficio del Software Libero in questo contesto è dato dal fatto che, dal momento che si ha il diritto di migliorare il software IA e condividere questi miglioramenti con altri, ciascuno può aumentarne la trasparenza, prevenendo pertanto la dipendenza dai fornitori dove si dovrebbe aspettare che chi fornisce il software renda l'IA più trasparente.</p>

  <h2 id="fairness">Imparzialità</h2>

  <p>Nell'ambito dell'intelligenza artificiale (IA), l'imparzialità è definita nel senso di rendere l'IA libera da pericolose discriminazioni basate sulle caratteristiche sensibili di un individuo, come il genere, l'etnia, la religione, le disabilità o l'orientamento sessuale. Dal momento che i modelli IA sono addestrati su insiemi di dati che contengono comportamenti e attività umane che possono non essere imparziali e che i modelli IA sono progettati per riconoscere e riprodurre schemi esistenti, è possibile creare discriminazioni dannose e violazioni dei diritti umani. Ad esempio, si è scoperto che (COMPAS)<a href="#fn-10" id="ref-10" class="fn">10</a>, un algoritmo che attribuisce un punteggio che indica quanto sia probabile che una persona sia recidiva sul proprio crimine, non fosse imparziale verso gli afroamericani<a href="#fn-11" id="ref-11" class="fn">11</a>, perché per essi il 44.9% dei casi erano falsi positivi. L'algoritmo attribuiva un'alta possibilità di recidiva nonostante gli accusati non reiterassero i crimini. Al contrario, il 47.7% dei casi per persone di razza bianca sono stati classificati a basso rischio di recidiva benché il crimine fosse poi stato reiterato. Una sospetta mancanza di imparzialità è stata trovata anche nell'assistenza sanitaria<a href="#fn-12" id="ref-12" class="fn">12</a>, dove un algoritmo è stato utilizzato per attribuire un fattore di rischio ai pazienti, identificando di conseguenza il bisogno di assegnargli risorse curative aggiuntive. Per avere lo stesso fattore di rischio dei bianchi, le persone di colore dovevano essere in una situazione di salute peggiore, in termini di gravità nell'ipertensione, nel diabete, nell'anemia, nell'alto livello di colesterolo o nell'insufficienza renale. Esistono quindi problemi reali di imparzialità negli algoritmi IA. Per di più, da una prospettiva legale, il controllo di problematiche di imparzialità è indicato dal considerando 71 del GDPR, che richiede di «<em>impedire tra l'altro effetti discriminatori nei confronti di persone fisiche sulla base della razza o dell'origine etnica, delle opinioni politiche, della religione o delle convinzioni personali, dell'appartenenza sindacale, dello status genetico, dello stato di salute o dell'orientamento sessuale, ovvero che comportano misure aventi tali effetti</em>». Abbiamo quindi bisogno di soluzioni per identificare potenziali problemi di imparzialità nell'insieme dei dati utilizzato per addestrare l'IA, e correggerli nel caso dovessero insorgere.</p>
  <p>Per determinare l'imparzialità, c'è bisogno di quantificarla. Ci sono molti modi per definire l'imparzialità nell'IA, che ricadono in due diversi tipi di approcci. Il primo verifica che raggruppamenti di persone secondo alcune caratteristiche sensibili vengano trattati in modo simile dall'algoritmo, ad esempio in termini di accuratezza, di tasso di veri positivi e di falsi positivi. Il secondo approccio misura l'imparzialità a livello individuale assicurandosi che persone simili siano trattate in modo simile dall'algoritmo<a href="#fn-13" id="ref-13" class="fn">13</a>. Più formalmente, vengono confrontate la distanze misurate tra i campioni dell'insieme di dati e le distanze misurate tra le previsioni dell'algoritmo, per assicurarsi che il loro rapporto sia coerente. Purtroppo, soddisfare l'imparzialità di gruppo e l'imparzialità individuale allo stesso tempo potrebbe non essere possibile<a href="#fn-14" id="ref-14" class="fn">14</a>. Ci sono tre metodi normalmente utilizzati per mitigare i problemi di imparzialità quando questi vengano riscontrati:
  </p>
  <ol>
    <li>Togliere gli attributi sensibili (ad esempio il genere, l'etnia, la religione, ecc.)
      dall'insieme dei dati. Questo approccio non funziona in uno scenario reale perché rimuovere gli attributi sensibili non è sufficiente per mascherarli completamente, dal momento che gli attributi sensibili sono spesso correlati con altri attributi dell'insieme dei dati. Eliminarli non è quindi sufficiente, e rimuovere tutti gli attributi correlati ad essi porta ad una perdita di informazioni troppo grande.</li>
    <li>Assicurarsi che l'insieme dei dati abbia una uguale rappresentazione delle persone se raggruppate da caratteristiche sensibili.</li>
    <li>Ottimizzare il modello IA sia per accuratezza che per imparzialità allo stesso tempo. Mentre l'algoritmo è addestrato su un esistente insieme di dati che contiene una non imparziale discriminazione, occorre considerare sia la sua accuratezza che la sua imparzialità<a href="#fn-15" id="ref-15" class="fn">15</a>. In altre parole, occorre aggiungere l'imparzialità come obiettivo dell'algoritmo.
    </li>
  </ol>
  <p>Se questi metodi vengono utilizzati, avere un algoritmo perfettamente accurato e imparziale è impossibile, ma se l'accuratezza è definita su un insieme di dati per il quale si sa che contiene un trattamento non imparziale di un gruppo particolare, allora può essere ritenuto accettabile avere un'accuratezza un po' meno che perfetta.
  </p>
  <p>Dal momento che una IA rilasciata come Software Libero può essere utilizzata ed analizzata da chiunque, è più semplice verificare che essa sia priva di potenziali discriminazioni dannose rispetto ad una IA proprietaria. Inoltre, questo crea sinergia con la trasparenza dell'IA (vedi la sezione <a href="#transparency">Transparenza</a>), dal momento che una IA trasparente facilita la comprensione dei fattori presi in considerazione per fare le previsioni. Benché sia necessario, rilasciare l'IA come Software libero non la rende comunque imparziale, ma rende l'imparzialità più semplice da valutare e da far rispettare.</p>

  <h2 id="conclusions">Conclusioni</h2>

  <p>In questo articolo, evidenziamo le potenziali problematiche sulla democratizzazione dell'intelligenza artificiale (IA) e le implicazioni per i diritti umani. Sono state presentate possibili soluzioni di Software Libero per affrontare queste problematiche. In particolare, abbiamo mostrato che l'IA ha bisogno di essere accessibile, trasparente e imparziale per poter essere utilizzabile. Se da una parte non è una soluzione sufficiente, rilasciare l'IA con licenze di Software Libero è necessario per l'uso diffuso che se ne fa nei nostri sistemi informativi, per poterla rendere più comprensibile, affidabile e sicura per tutti.</p>

  <h2 id="fn">References</h2>
  <ol>
    <li id="fn-1">K. He, X. Zhang, S. Ren, e J. Sun, “Deep Residual Learning for Image Recognition,” <em>arXiv:1512.03385 [cs]</em>, Dic. 2015. <a href="#ref-1">↩</a></li>
    <li id="fn-2">K. Simonyan e A. Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recognition,” <em>arXiv:1409.1556 [cs]</em>, Apr. 2015. <a href="#ref-2">↩</a></li>
    <li id="fn-3">X. Dastile, T. Celik, e M. Potsane, “Statistical and machine learning models in credit scoring: A systematic literature survey,” <em>Applied Soft Computing</em>, vol. 91, p. 106263, 2020, doi: <a href="https://doi.org/10.1016/j.asoc.2020.106263">10.1016/j.asoc.2020.106263</a>. <a href="#ref-3">↩</a></li>
    <li id="fn-4">C. Badue <em>et al.</em>, “Self-Driving Cars: A Survey,” <em>arXiv:1901.04407 [cs]</em>, Oct. 2019. <a href="#ref-4">↩</a></li>
    <li id="fn-5">D. Ensign, S. A. Friedler, S. Neville, C. Scheidegger, e S. Venkatasubramanian, “Runaway Feedback Loops in Predictive Policing,” in <em>Conference on Fairness, Accountability and Transparency</em>, Jan. 2018, pp. 160–171. <a href="#ref-5">↩</a></li>
    <li id="fn-6">N. Schwalbe and B. Wahl, “Artificial intelligence and the future of global health,” <em>The Lancet</em>, vol. 395, no. 10236, pp. 1579–1586, Mag. 2020, doi: <a href="https://doi.org/10.1016/S0140-6736(20)30226-9">10.1016/S0140-6736(20)30226-9</a>. <a href="#ref-6">↩</a></li>
    <li id="fn-7">A. Canziani, A. Paszke, e E. Culurciello, “An Analysis of Deep Neural Network Models for Practical Applications,” <em>arXiv:1605.07678 [cs]</em>, Apr. 2017. <a href="#ref-7">↩</a></li>
    <li id="fn-8">M. T. Ribeiro, S. Singh, e C. Guestrin, “"Why Should I Trust You?": Explaining the Predictions of Any Classifier,” <em>arXiv:1602.04938 [cs, stat]</em>, Ago. 2016. <a href="#ref-8">↩</a></li>
    <li id="fn-9">N. Kokhlikyan <em>et al.</em>, <em>Captum: A unified and generic model interpretability library for PyTorch</em>. 2020. <a href="#ref-9">↩</a></li>
    <li id="fn-10">“Practitioners Guide to COMPAS.” Northpointe, Mar. 2015. <a href="#ref-10">↩</a></li>
    <li id="fn-11">L. K. Mattu Jeff Larson, “Machine Bias,” <em>ProPublica</em>. Mar. 2015. <a href="#ref-11">↩</a></li>
    <li id="fn-12">Z. Obermeyer, B. Powers, C. Vogeli, e S. Mullainathan, “Dissecting racial bias in an algorithm used to manage the health of populations,” <em>Science (New York, N.Y.)</em>, vol. 366, no. 6464, pp. 447–453, Ott. 2019, doi: <a href="https://doi.org/10.1126/science.aax2342">10.1126/science.aax2342</a>. <a href="#ref-12">↩</a></li>
    <li id="fn-13">C. Dwork, M. Hardt, T. Pitassi, O. Reingold, e R. Zemel, “Fairness Through Awareness,” <em>arXiv:1104.3913 [cs]</em>, Nov. 2011. <a href="#ref-13">↩</a></li>
    <li id="fn-14">J. Kleinberg, S. Mullainathan, and M. Raghavan, “Inherent Trade-Offs in the Fair Determination of Risk Scores,” <em>arXiv:1609.05807 [cs, stat]</em>, Nov. 2016. <a href="#ref-14">↩</a></li>
    <li id="fn-15">M. B. Zafar, I. Valera, M. G. Rodriguez, e K. P. Gummadi, “Fairness Beyond Disparate Treatment &amp; Disparate Impact: Learning Classification without Disparate Mistreatment,” <em>Proceedings of the 26th International Conference on World Wide Web</em>, pp. 1171–1180, Apr. 2017, doi: <a href="https://doi.org/10.1145/3038912.3052660">10.1145/3038912.3052660</a>. <a href="#ref-15">↩</a></li>
  </ol>

</body>
<author id="lequertier"/>
<date>
  <original content="2021-04-17"/>
</date>
<sidebar/>
<translator>Francesco Florian e Luca Bonissi</translator>
</html>
